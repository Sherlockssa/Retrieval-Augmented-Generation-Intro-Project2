{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bff53e",
   "metadata": {},
   "source": [
    "# Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60175c42-62b9-4eca-a1e1-2a5846d1fc9a",
   "metadata": {},
   "source": [
    "In this notebook we will look into building an basic RAG pipeline with LlamaIndex. It has following 2 sections.\n",
    "\n",
    "1. Understanding Retrieval Augmented Generation (RAG).\n",
    "2. Building basic RAG with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9fd12c-8c5e-452d-bcf9-01e1c0eb6fbe",
   "metadata": {},
   "source": [
    "**Retrieval Augmented Generation (RAG)**\n",
    "\n",
    "LLMs are trained on vast datasets, but these will not include your specific data. Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data during the generation process. This is done not by altering the training data of LLMs, but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\n",
    "\n",
    "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
    "\n",
    "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd18319-426e-4bdc-892b-49c38750976f",
   "metadata": {},
   "source": [
    "![RAG Overview](../data/llamaindex_rag_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39061c-f0fa-443b-8b76-4a1fc0ce9e90",
   "metadata": {},
   "source": [
    "**Stages within RAG**\n",
    "\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "\n",
    "**Loading:** this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
    "\n",
    "**Indexing:** this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "\n",
    "**Storing:** Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\n",
    "\n",
    "**Querying:** for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "**Evaluation:** a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are. However, this part is not covered in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa1339-3e31-46cd-9ba4-a58165feeda6",
   "metadata": {},
   "source": [
    "## Build RAG system.\n",
    "\n",
    "Now that we have understood the significance of RAG system, let's build a simple basci RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a850ab0-6d83-493a-a829-956e146c64e1",
   "metadata": {},
   "source": [
    "#### Load Data and Build Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101152e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../data/Henry.txt\"]\n",
    ").load_data()\n",
    "\n",
    "local_model_path = \"/data-extend/zhengwenhao/workspace/RAG/tex2vec/bge-base-zh-v1.5\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=local_model_path)\n",
    "\n",
    "# 创建 VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9c68c-e188-484e-b685-55047ea7b45f",
   "metadata": {},
   "source": [
    "Build a QueryEngine and start querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52a26c-7d0c-44df-8043-4c7f19f794b9",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from load_llama2 import OurLLM\n",
    "Settings.llm = OurLLM()\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d5b6e-cc2e-4648-b28c-5fa25a97d175",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Who is the pretty boy in Hong Kong?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bedda-6483-416a-aa78-40122316c954",
   "metadata": {},
   "source": [
    "By default it retrieves `two` similar nodes/ chunks. You can modify that in `vector_index.as_query_engine(similarity_top_k=k)`.\n",
    "\n",
    "**Let's check the text in each of these retrieved nodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc83e5-97be-4fa8-b97f-26783bba251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First retrieved node\n",
    "response.source_nodes[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980292a-e8f7-4c8b-8d9b-c3072500fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second retrieved node\n",
    "response.source_nodes[1].get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
